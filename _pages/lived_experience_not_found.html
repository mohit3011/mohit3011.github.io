---
layout: archive
title: ""
permalink: /lived_experience_not_found_blog/
author_profile: true
---


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ADR Paper Blog</title>
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- Bulma CSS Framework -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            color: #363636;
        }

        .container {
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
            padding: 20px;
            margin-top: 20px;
            max-width: 1200px;
            margin: 0 auto;
            width: 90%;
        }

        h1, h2, h3 {
            color: #2e2e2e;
            margin-bottom: 15px;
        }

        h1 {
            font-size: clamp(1.5rem, 4vw, 2rem);
            font-weight: 700;
            text-align: center;
            margin: 2rem 0;
            line-height: 1.4;
            color: #2e2e2e;
            padding: 0 1rem;
        }

        p, a {
            font-size: 1rem;
            line-height: 1.8;
        }

        a {
            color: #3273dc;
            text-decoration: none;
            font-weight: 500;
        }

        a:hover {
            text-decoration: underline;
        }

        .authors, .links {
            margin: 20px 0;
            padding: 15px;
            border-left: 5px solid #3273dc;
            border-radius: 4px;
        }

        .authors h3, .links h3 {
            margin-bottom: 10px;
        }

        .links ul {
            list-style-type: none;
        }

        .links li {
            margin-bottom: 8px;
        }

        .buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 20px;
            flex-direction: row;
            flex-wrap: wrap;
            padding: 0 1rem;
        }

        .button {
            display: flex;
            align-items: center;
            justify-content: center;
            padding: 10px 20px;
            border-radius: 5px;
            text-decoration: none;
            font-weight: 500;
            color: #fff;
            font-size: 1rem;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: background-color 0.3s ease;
            white-space: nowrap;
            margin: 5px;
        }

        .button.arxiv {
            background-color: #B31B1B;
        }

        .button.github {
            background-color: #333;
        }

        .button img {
            margin-right: 10px;
            width: 20px;
            height: 20px;
        }

        .button:hover {
            opacity: 0.9;
        }

        footer {
            text-align: center;
            margin-top: 40px;
            font-size: 0.9rem;
        }

        .authors {
            margin: 2rem clamp(0.5rem, 3vw, 2rem);
            padding: 1.5rem clamp(1rem, 3vw, 1.5rem);
            background-color: #f8f9fa;
            border-radius: 8px;
            border-left: none;
        }

        .author-list {
            margin-bottom: 1rem;
            line-height: 1.6;
            font-size: clamp(0.9rem, 2.5vw, 1rem);
        }

        .author-name {
            color: #2a2a2a;
            text-decoration: none;
            transition: color 0.2s ease;
        }

        .author-name:hover {
            color: #3273dc;
            text-decoration: none;
        }

        .affiliations {
            font-size: clamp(0.8rem, 2vw, 0.9rem);
            color: #666;
            line-height: 1.4;
        }

        .affiliation {
            display: inline-block;
            margin-right: 0.5rem;
        }

        .affiliation sup {
            color: #3273dc;
            font-weight: 500;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 600;
            color: #2e2e2e;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-top: 2px solid #eaeaea;
        }

        .figure {
            width: min(90%, 800px);
            padding: 0 1rem;
            text-align: center;
        }

        .figure img {
            width: 100%;
            height: auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 0 auto;
            display: block;
        }

        .figure figcaption {
            margin-top: 1rem;
            font-size: 0.9rem;
            color: #666;
            line-height: 1.4;
            font-style: italic;
            text-align: center;
            width: 100%;
        }

        /* Media Queries */
        @media screen and (max-width: 768px) {
            .buttons {
                flex-direction: column;
                align-items: stretch;
            }

            .button {
                width: 100%;
                margin: 5px 0;
            }

            .figure {
                width: 100%;
            }

            .affiliations p {
                display: flex;
                flex-direction: column;
                gap: 0.5rem;
            }

            .affiliation {
                display: block;
            }

            /* Hide the separator on mobile */
            .affiliations span.affiliation + span::before {
                display: none;
            }
        }

        @media screen and (max-width: 480px) {
            .author-list p {
                display: flex;
                flex-direction: column;
                gap: 0.5rem;
            }

            .author-list a::after {
                content: ',';
                display: none;
            }
        }

        h3 {
            font-size: 1.25rem;
            font-weight: 600;
            color: #2e2e2e;
            margin-top: 2rem;
            margin-bottom: 1rem;
            line-height: 1.4;
        }

        h3:contains("Finding") {
            color: #3273dc;
            font-weight: 500;
        }

        .citation {
            background-color: #1e2023;
            color: #eceff4;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Fira Code', 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            border: 1px solid #3b4252;
            white-space: pre-wrap;
            position: relative;
        }

        .copy-button {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: #3273dc;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 8px;
            transition: background-color 0.3s ease;
            z-index: 1;
        }

        .copy-button:hover {
            background-color: #2366d1;
        }
    </style>
</head>
<body>

    <!-- Main Content -->
    <div>
        <h1><i>Lived Experience Not Found:</i> LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions</h1>
        
        <div class="authors">
            <h3>Authors</h3>
            <div class="author-list">
                <p>
                    <a href="#" class="author-name">Mohit Chandra</a><sup>1</sup>, 
                    <a href="#" class="author-name">Siddharth Sriraman</a><sup>1</sup>, 
                    <a href="#" class="author-name">Gaurav Verma</a><sup>1</sup>, 
                    <a href="#" class="author-name">Harneet Singh Khanuja</a><sup>1</sup>, 
                    <a href="#" class="author-name">Jose Suarez Campayo</a><sup>2</sup>, 
                    <a href="#" class="author-name">Zihang Li</a><sup>3</sup>, 
                    <a href="#" class="author-name">Michael L. Birnbaum</a><sup>4</sup>, 
                    <a href="#" class="author-name">Munmun De Choudhury</a><sup>1</sup>
                </p>
            </div>
            <div class="affiliations">
                <p><span class="affiliation"><sup>1</sup>College of Computing, Georgia Institute of Technology</span> | 
                   <span class="affiliation"><sup>2</sup>Hospital General Universitario Gregorio Marañón</span> | 
                   <span class="affiliation"><sup>3</sup>Hofstra University</span> | 
                   <span class="affiliation"><sup>4</sup>Columbia University</span></p>
            </div>
        </div>
        
        <div>
            <div class="buttons">
                <a href="https://arxiv.org/abs/2310.13132" target="_blank" class="button arxiv">
                    <i class="fas fa-file-pdf"></i>
                    Paper on arXiv
                </a>
                <a href="https://github.com/claws-lab/XLingEval" target="_blank" class="button github">
                    <i class="fab fa-github"></i>
                    Code Repository
                </a>
            </div>
        </div>

        <h2>Introduction</h2>
        <div>
            <figure class="figure" style="width: 100%; text-align: center;">
                <img src="../images/adr_paper_blog/figure_1.png" alt="Overview of work; we present two tasks in this work– ADR detection and multiclass classification (RQ1), and Expert-LLM response alignment (RQ2)." style="width: 40%; margin: 0 auto; display: block;" />
                <figcaption>Figure 1: Overview of work; we present two tasks in this work– ADR detection and multiclass classification (RQ1), and Expert-LLM response alignment (RQ2).</figcaption>
            </figure>
            <br>
            <p>Adverse Drug Reactions (ADRs) from psychiatric medications are one of the leading causes of hospitalizations among mental health patients. With healthcare systems and online communities facing limitations in resolving ADR-related issues, Large Language Models (LLMs) have the potential to fill this gap. However, despite the increasing capabilities of LLMs, past research has not explored their capabilities in detecting ADRs related to psychiatric medications or in providing effective harm reduction strategies. To address this, in this work, we introduce the Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment (ADRA) framework to systematically evaluate LLM performance in detecting ADR expressions and delivering expert-aligned mitigation strategies. The proposed Psych-ADR benchmark includes 239 Reddit posts, labeled across two hierarchical levels for ADR detection and multiclass classification along with expert-written responses to queries. The proposed framework evaluates LLM-generated responses against those of medical experts, focusing on four assessment axes: (a) text readability, (b) emotion and tone expression, (c) alignment of harm-reduction strategies, and (d) actionability of suggested strategies.</p>
        </div>

        <h2>Psych-ADR Benchmark</h2>

        <div>
            <figure class="figure" style="width: 100%; text-align: center;">
                <img src="../images/adr_paper_blog/figure_2.png" alt="Figure 2: (Left)Class-wise distribution of examples in thePsych-ADR benchmark dataset; % w.r.t. N = 239. (Right) Sample answer representing the structure of answers provided in the Psych-ADR benchmark dataset." style="width: 60%; margin: 0 auto; display: block;" />
                <figcaption>Figure 2: (Left)Class-wise distribution of examples in thePsych-ADR benchmark dataset; % w.r.t. N = 239. (Right) Sample answer representing the structure of answers provided in the Psych-ADR benchmark dataset.</figcaption>
            </figure>
            <br>
            <p>Psych-ADR benchmark is a novel dataset with 239 reddit posts related to psychiatric medications and symptoms with annotations and replies provided by medical experts. The dataset contains two-hierarchical labels depicting -- 1) Presence/Absence of ADR, 2) Category of ADR. A key aspect of the Psych-ADR benchmark is the inclusion of expert-written responses to queries in the ADR labeled posts. We identified and articulated the logical structure where each response in our dataset begins with empathizing with the patient, followed by information on diagnosis, request for additional information, proposing harm reduction strategies to mitigate the ADR, and concluding with a final set of questions.</p>
        </div>

        <h2>ADRA Alignment Framework</h2>

        <div>
            <p>Evaluation of long-form text generation is an open problem and involves many challenges like isolating the stylistics from the semantics. However, in the context of responses to ADR queries, we propose abstracting out the LLM generations and ground-truth expert responses to four key components– (1) emotion and tone, (2) text readability, (3) harm reduction strategy, and (4) actionability of proposed strategies. Via this abstraction to key components, our alignment evaluations focus on specific aspects that contribute towards an ideal response to ADR queries.</p>
            <br>
            <ul style="margin-left: 40px;">
                <li><b>Emotion and Tone Alignment</b>: We measured the alignment of LLM-generated and expert written answers across 8 relevant emotional and tonal categories identified from prior literature.</li>
                <br>
                <li><b>Readability Alignment</b>: To evaluate the difficulty level of reading both LLM-generated content and expert-written text, we employed the SMOG Index—a widely recognized measure commonly used to assess health literacy materials.</li>
                <br>
                <li><b>Harm Reduction Strategy Alignment</b>: In cases of adverse reactions to psychiatric medications, suggesting safe medical interventions is crucial to prevent further harm. We operationalized these interventions using harm reduction strategies (HRS), aimed at minimizing the negative effects of medications that one is reliant on. Ideally, LLMs should propose strategies that align with the expert's responses.</li>
                <br>
                <li><b>Actionability of Proposed Strategies</b>: Actionability in the responses of healthcare professionals enables greater engagement and encourages increased action from patients. To this end, we designed an approach to measure the alignment between LLM responses and expert responses along the actionability dimension using four (4) sub-criteria.</li>
            </ul>
        </div>

        <h2>Findings</h2>

        <div>
            <figure class="figure" style="width: 100%; text-align: center;">
                <img src="../images/adr_paper_blog/figure_3.png" alt="Table 1: Performance of different models on Binary Detection and Multiclass Classification tasks under Zero-Shot and 5-Shot scenarios. We report the accuracy score(Acc.) and weighted F1 score as(F1) with the best and second-best performing model metrics in each scenario highlighted in bold and underline, respectively." style="width: 60%; margin: 0 auto; display: block;" />
                <figcaption>Table 1: Performance of different models on Binary Detection and Multiclass Classification tasks under Zero-Shot and 5-Shot scenarios. We report the accuracy score(Acc.) and weighted F1 score as(F1) with the best and second-best performing model metrics in each scenario highlighted in bold and underline, respectively.</figcaption>
            </figure>
        </div>

        <h3>Finding 1: Larger models typically perform better for ADR detection tasks,but this trend does not hold for ADR multiclass classification</h3>

        <div>
            <p>As expected, larger models(by parameter size) outperformed their smaller counterparts in the ADR detection task within their respective families,with Claude3 Opus achieving the highest accuracy at 77.41%, followed by GPT-4o and GPT-4Turbo at 72.03%. Interestingly, specialized medical models(OpenBioLLM-Llama3-70B and Llama3Med42v2-70B)struggled in this task. However, for ADR multiclass classification,we did not observe any clear pattern between model size and performance. GPT-4 Turbo was the best performing model with an accuracy of 57.58%, followed by Llama3-Med42v2-70B at 56.40%. All models struggled with multiclass classification,likely due to the complexity of distinguishing between ADR types.</p>
        </div>

        <h3>Finding 2: Models exhibited a “risk-averse” tendency, and prone to commit false-positive errors</h3>

        <div>
            <p>In both ADR detection and multiclass classification tasks, all models displayed “risk-averse" behavior, often mislabeling posts without ADRs as positive for ADRs. In zero-shot settings,Claude Opus had a false-positive rate of 42% for ‘ADR-No’ labels, while Claude3 Haiku’s false positive rate was as high as 97%(see paper). Similarly, in ADR multiclass classification, models struggled to distinguish between non-dose-related, dose-related, and time-related ADRs. GPT-4Turbo misclassified 51% of non-dose-related and 50% of time-related ADRs in zero-shot settings.</p>
        </div>

        <h3>Finding 3: In-context learning enhances model performance but not in every case</h3>

        <div>
            <p>We observed that in-context learning in general improved performance of models for both ADR detection and multiclass classification tasks, with a more significant impact on the latter task. For multiclass classification, we observed an average increase of 18.14 and 23.06 points in weighted F1 score among model performance using least-similar and most-similar example prompting respectively. However, this pattern was not observed in the ADR detection task. Claude 3 Opus outperformed other models in the ADR detection, achieving an F1 score of 76.44 with zero-shot prompting. In ADR multiclass classification, Llama-3.5-405B performed best with most-similar examples (F1 76.69).</p>
        </div>

        <h3>Finding 4: LLMs generate harder to read replies as compared to experts</h3>

        <div>
            <figure class="figure" style="width: 100%; text-align: center;">
                <img src="../images/adr_paper_blog/figure_4.png" alt="Figure 3: Mean SMOG Scores and 95% Confidence Intervals for Various Models (lower values are better)." style="width: 60%; margin: 0 auto; display: block;" />
                <figcaption>Figure 3: Mean SMOG Scores and 95% Confidence Intervals for Various Models (lower values are better).</figcaption>
            </figure>
            <br>
            <p>LLM-generated responses tend to be more complex, reflected in higher SMOG scores compared to those written by experts (SMOG mean11.02) in the Psych-ADR benchmark. Welch’s t-test (Welch, 1947) further revealed that the SMOG scores of expert-written responses were significantly lower than those of any LLMgenerated responses. We also observed that more capable models produced more readable responses (with Claude 3 Opus being an exception). Similar to the findings on emotional alignment, Llama3Med42v2-70B showed the lowest alignment with the expert-written responses, producing the most complex responses, likely due to a major portion of instruction-tuning data coming from medical and biomedical scientific literature.</p>
        </div>

        <h3>Finding 5: LLMs struggle to provide expert-aligned harm reduction strategies with the best model providing ~71% alignment with experts</h3>

        <div>
            <figure class="figure" style="width: 100%; text-align: center;">
                <img src="../images/adr_paper_blog/figure_5.png" alt="Table 2: Alignment of harm reduction strategies of various models with the expert’s response.We report the mean and standard deviation for the AlignScore metric GPT-4o score, with the best (bold) and second-best (underline) performing model in each metric highlighted." style="width: 60%; margin: 0 auto; display: block;" />
                <figcaption>Table 2: Alignment of harm reduction strategies of various models with the expert’s response.We report the mean and standard deviation for the AlignScore metric GPT-4o score, with the best (bold) and second-best (underline) performing model in each metric highlighted.</figcaption>
            </figure>
            <br>
            <p>More capable models Llama 3.1-405B Instruct and Claude 3.5 Sonnet in their respective families tended to produce strategies less aligned with the expert than their smaller counterparts, validating a previously observed pattern of LLM performance in responding to open-ended clinical questions (Kanithi et al., 2024). While the open-weights models performed on par or better than proprietary models across both alignment metrics, the best performing medical model (OpenBioLLM-Llama3-70B) aligned with expert harm reduction strategies for 70.86% of the cases, highlighting the need for further fine-tuning for specialized domains such as psychiatry.</p>
        </div>

        <h3>Finding 6: While LLMs provide less practical and relevant advice, their advice is more clear and specific.</h3>

        <div>
            <figure class="figure" style="width: 100%; text-align: center;">
                <img src="../images/adr_paper_blog/figure_6.png" alt="Table 3: Mean actionability alignment scores of harm reduction strategies (last column),computed as average of practicality, relevance, specificity, and clarity scores." style="width: 60%; margin: 0 auto; display: block;" />
                <figcaption>Table 3: Mean actionability alignment scores of harm reduction strategies (last column),computed as average of practicality, relevance, specificity, and clarity scores.</figcaption>
            </figure>
            <br>
            <p>We noted that expert responses scored the highest on overall actionability in comparison to all the LLMs(0.46). Nonetheless, medical models like OpenBioLLMLlama3-70B and Llama3-Med42v2-70B demonstrated reasonable actionability scores(0.44), followed by other proprietary and open-weights models (0.35 to 0.43). While expert responses were rated considerably better than all LLM responses in terms of the practicality (0.83) and contextual relevance (0.73) of the harm reduction strategies,their specificity (0.17) and clarity (0.13) are relatively lacking. This indicates that while LLMs tend to demonstrate greater specificity and clarity in their harm reduction strategy, the recommended strategies may often not be feasible and contextually relevant,considering the users personal circumstances,such as physical ability,financial resources, and time constraints.</p>
        </div>

        <h2>Broader Implications of Our Work</h2>

        <h3>Need for going beyond the choice-based medical benchmarks</h3>

        <div>
            <p>LLMs have achieved near-perfect scores on popular medical benchmarks (Nori et al., 2023a; Singhal et al., 2023b), however, these evaluations typically focus on multiple-choice or case based questions,which don’t reflect the nuanced understanding required in real-world scenarios like mental health. Despite their strong performance on medical tasks, Llama3-Med42v2-70B and OpenBioLLM-Llama3-70B struggled with detecting ADRs and providing aligned and actionable HRS, highlighting the need to move beyond standard benchmarks towards more holistic alignment evaluation paradigms.</p>
        </div>

        <h3>Focusing on empowering experts rather than replacing them</h3>

        <div>
            <p>While LLMs did not match expert performance in our analysis, they showed a potential to enhance healthcare by providing clearer, more actionable responses. Given the global shortage of mental health professionals (Kazdin and Rabbitt, 2013), LLMs could expand access to mental healthcare and support experts with further finetuning and alignment with expert reasoning.</p>
        </div>

        <h3>Disentangling inclusion of humanistic features in LLMs and advocacy for inclusion of lived experience</h3>

        <div>
            <p>While our work provides evidence of the lack of lived experience, which is essential for understanding the nuances of a complex task such as ADR detection and for proposing mitigation strategies in model responses, we do not advocate for increasing human-like features in LLMs. Previous studies have suggested that heightened anthropomorphism, independent of whether it is accompanied by enhanced capabilities, can increase trust among individuals (Natarajan et al., 2020; Chen et al., 2021). In contrast, we advocate for approaches that align with previous research, which has shown that the efficacy of LLMs in the healthcare domain can be enhanced through fine-tuning on specialized data or by incorporating useful features into the model, without introducing human-like features.</p>
        </div>

        <h2>Citation</h2>
        <div class="citation-container">
                <pre id="citation-text" class="citation">
                <button onclick="copyToClipboard()" class="copy-button">
                    <i class="far fa-copy"></i> Copy
                </button>@article{chandra2024lived,
                title     = {Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use},
                author    = {Chandra, Mohit and Sriraman, Siddharth and Verma, Gaurav and Khanuja, Harneet Singh and Campayo, Jose Suarez and Li, Zihang and Birnbaum, Michael L and De Choudhury, Munmun},
                journal   = {arXiv preprint arXiv:2410.19155},
                year      = {2024}
            }</pre>
        </div>

        <script>
            function copyToClipboard() {
                // Create temporary textarea
                const textarea = document.createElement('textarea');
                textarea.value = `@article{chandra2024lived,
    title     = {Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use},
    author    = {Chandra, Mohit and Sriraman, Siddharth and Verma, Gaurav and Khanuja, Harneet Singh and Campayo, Jose Suarez and Li, Zihang and Birnbaum, Michael L and De Choudhury, Munmun},
    journal   = {arXiv preprint arXiv:2410.19155},
    year      = {2024}
}`;
                document.body.appendChild(textarea);
                textarea.select();
                
                try {
                    document.execCommand('copy');
                    const button = document.querySelector('.copy-button');
                    button.style.backgroundColor = '#48c774';
                    button.innerHTML = '<i class="fas fa-check"></i> Copied';
                    
                    setTimeout(() => {
                        button.style.backgroundColor = '#3273dc';
                        button.innerHTML = '<i class="far fa-copy"></i> Copy';
                    }, 2000);
                } catch (err) {
                    console.error('Failed to copy');
                }
                
                document.body.removeChild(textarea);
            }
        </script>

        <footer>
            &copy; 2025 ADR Paper Blog. All rights reserved.
        </footer>
    </div>
</body>
</html>